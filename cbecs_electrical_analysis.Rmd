---
title: "cbecs_electrical_analysis"
author: "John Grando"
date: "February 11, 2019"
output: html_document
---

Source files
```{r}
source('tree_func.R')
source('cbecs_2012_clean_transform.R')
```

Libraries
```{r}
library(factoextra)
library(pls)
library(randomForest)
library(inTrees)
library(doParallel)
library(elasticnet)
library(keras)
library(tibble)
library(RSNNS)
```
#Initialize parallel processing on 2 cores
```{r}
cl <- makePSOCKcluster(2)
registerDoParallel(cl)
```

Load and transform
```{r}
cbecs_raw_df <- read.csv('./2012_public_use_data_aug2016.csv', 
                         header = TRUE, 
                         stringsAsFactors = FALSE)
cbecs_dfs <- clean_encode_cbecs(cbecs_raw_df)
```

### Explore

```{r}
#remove buildings that know the don't use electricity
cbecs_el_encoded_df <- cbecs_dfs$encoded_df %>% filter(ELUSED.1==1)
cbecs_el_cleaned_df <- cbecs_dfs$clean_df %>% filter(ELUSED==1) 
```

Response Variable
```{r}
lmbda <- BoxCoxTrans(cbecs_el_cleaned_df$ELBTUPerSf + 1)$lambda
ggplot(cbecs_el_cleaned_df, aes(x=ELBTUPerSf^lmbda)) + 
  geom_density() + 
  stat_function(fun=dnorm,
                color="blue",
                args=list(
                  mean=mean(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE),
                  sd(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE)
                )
  ) +
  ggtitle(paste('ELBTUPerSf Transformed via BoxCox, Lambda = ', lmbda, sep=""))
```

Remove highly correlated pairs and report???
```{r warning=FALSE}
cbecs_elbtu_encoded_df <- cbecs_el_encoded_df %>% 
  #remove zero variance variables
  select(-one_of(preProcess(., method = c('zv'))$method$remove)) %>% 
  select(-one_of(cbecs_dfs$response_cols)) %>% 
  select(-one_of(findCorrelation(cor(.), cutoff=0.75, names = TRUE))) %>%
  #remove remaining pba values then reinsert all
  select(-matches('^PBA')) %>% 
  bind_cols(cbecs_el_encoded_df %>% select(matches('^PBA\\.'))) %>% 
  bind_cols(ELBTUPerSf = cbecs_el_encoded_df %>% select(ELBTUPerSf))

cbecs_elbtu_encoded_numerics_cols <- colnames(
  cbecs_elbtu_encoded_df %>% 
    select(one_of(cbecs_dfs$encoded_numeric_cols)) %>% 
    select(-ELBTUPerSf)) 
cbecs_elbtu_encoded_non_numerics_cols <- colnames(cbecs_elbtu_encoded_df %>% select(one_of(cbecs_dfs$encoded_non_numeric_cols)))

#center and scale only numeric columns in data set, not encoder columns
elbtu_pre_process <- preProcess(
  cbecs_elbtu_encoded_df, 
  method = list(center = cbecs_elbtu_encoded_numerics_cols, 
       scale = cbecs_elbtu_encoded_numerics_cols))
cbecs_elbtu_encoded_center_scale_df <- predict(elbtu_pre_process, cbecs_elbtu_encoded_df)
```

```{r}
cbecs_elbtu_df <- cbecs_el_encoded_df %>% 
  select(-one_of(cbecs_dfs$response_cols)) %>% 
  bind_cols(ELBTUPerSf = cbecs_el_encoded_df %>% select(ELBTUPerSf))
```

PCA
```{r fig.width=10}
cbecs_pca <- prcomp(cbecs_elbtu_encoded_center_scale_df %>% 
    select(-ELBTUPerSf) %>%
    select(-one_of(cbecs_dfs$response_cols)), center = TRUE, scale. = TRUE)
fviz_eig(cbecs_pca, addlabels = TRUE)
var <- get_pca_var(cbecs_pca)
fviz_pca_var(cbecs_pca, select.var = list(contrib=50), repel = TRUE)
fviz_contrib(cbecs_pca, choice = "var", axes = 1:5, top = 50)
pca_imp_df <- fviz_contrib(cbecs_pca, choice = "var", axes = 1:5, top = 50)$data %>% arrange(desc(contrib)) %>% head(20) %>% select(name) %>% rename(feature = name) %>% mutate(feature = as.character(feature))
```

#Train/Test split
```{r}
cbecs_train_list <- createDataPartition(y= cbecs_el_cleaned_df$PBA, p=0.8, list = FALSE) 
cbecs_elbtu_train_df <- cbecs_elbtu_df[cbecs_train_list,]
cbecs_elbtu_test_df <- cbecs_elbtu_df[-cbecs_train_list,]
cbecs_elbtu_encoded_train_df <- cbecs_elbtu_encoded_df[cbecs_train_list,]
cbecs_elbtu_encoded_test_df <- cbecs_elbtu_encoded_df[-cbecs_train_list,]
cbecs_elbtu_encoded_center_scale_train_df <- cbecs_elbtu_encoded_center_scale_df[cbecs_train_list,]
cbecs_elbtu_encoded_center_scale_test_df <- cbecs_elbtu_encoded_center_scale_df[-cbecs_train_list,]
#stratify training set
folds <- 10
cbecs_train_cv_list <- createFolds(y=cbecs_dfs$clean_df[cbecs_train_list,]$PBAPLUS, k = folds, returnTrain = TRUE)
```

#PLS 
```{r fig.height=14}
control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid', verboseIter = TRUE)
tunegrid <- expand.grid(ncomp = seq(1, 30, 3))
pls_train <- caret::train(
  y = cbecs_elbtu_encoded_center_scale_train_df$ELBTUPerSf^lmbda,
  x = cbecs_elbtu_encoded_center_scale_train_df %>% select(-ELBTUPerSf),
  method='pls',
  metric='RMSE',
  maximize=FALSE,
  tuneGrid=tunegrid,
  trControl=control
)
print(pls_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=pls_train$finalModel$fitted.values[,,10]^(1/lmbda), 
           y=ELBTUPerSf)) + geom_point()
varImp(pls_train)
pls_imp_df <- varImp(pls_train)$importance %>% rownames_to_column(var='feature') %>% arrange(desc(Overall)) %>% head(20) %>% select(feature)
plot(pls_train$finalModel$fitted.values[,,10]^(1/lmbda), pls_train$finalModel$residuals[,,10])
```

#Tree regression importance
```{r fig.height=10}
#eval set to false, takes a long time to process
control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid', verboseIter = TRUE)
tunegrid <- expand.grid(mtry = seq(10, 10, 50))
rf_train <- caret::train(
  ELBTUPerSf ~ .,
  data=cbecs_elbtu_encoded_train_df,
  method='rf',
  metric='Rsquared',
  tuneGrid=tunegrid,
  trControl=control,
  maxnodes=30,
  importance = TRUE
)
print(rf_train)
ggplot(cbecs_elbtu_train_df, 
       aes(x=rf_train$finalModel$predicted, 
           y=ELBTUPerSf)) + geom_point()
varImp(rf_train)
rf_imp_df <- varImp(rf_train)$importance %>% rownames_to_column(var='feature') %>% arrange(desc(Overall)) %>% head(20) %>% select(feature)
```

lasso
```{r eval=FALSE}
#This doesn't work and takes too long, use it as a reason for nn model

#Do lasso as well, then maybe try rfe (recursive feature elimination) for linear model, and classification tree, then compare their metrics together.  Mabye set up a custom one or also try lasso?

control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid', verboseIter = TRUE)
tunegrid <- expand.grid(fraction = seq(.01, .1, .02))
l_train <- caret::train(
  ELBTUPerSf^lmbda ~ .,
  data=cbecs_elbtu_encoded_center_scale_train_df,
  method='lasso',
  metric='Rsquared',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv')
)
print(l_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(l_train, newx = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df))^(1/lmbda),
           y=ELBTUPerSf)) + geom_point()
varImp(l_train)
```

leaps
```{r eval=FALSE}
#This doesnt work and takes too long, use ti as as reason for nn model

#Do lasso as well, then maybe try rfe (recursive feature elimination) for linear model, and classification tree, then compare their metrics together.  Mabye set up a custom one or also try lasso?

control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid', verboseIter = TRUE)
tunegrid <- expand.grid(nvmax = seq(100, 700, 100))
l_train <- caret::train(
  ELBTUPerSf^lmbda ~ .,
  data=cbecs_elbtu_encoded_center_scale_train_df,
  method='leapBackward',
  metric='Rsquared',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv')
)
print(l_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(l_train, newx = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df))^(1/lmbda),
           y=ELBTUPerSf)) + geom_point()
varImp(l_train)
```

Plots for high importance variables
```{r}
ggplot(cbecs_el_cleaned_df, aes(x=(RGSTRNPerSf+1), y=ELBTUPerSf^lmbda)) + geom_point()
#also do this for residuals on best fits...
```

#make neural network input df based on the most important features selected
```{r}
nn_input_train_test_df <- cbecs_elbtu_encoded_center_scale_train_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature),
         one_of(colnames(cbecs_elbtu_encoded_center_scale_df %>% select(matches('^PBA\\.'), ELBTUPerSf))))

nn_input_validate_df <- cbecs_elbtu_encoded_center_scale_test_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature),
         one_of(colnames(cbecs_elbtu_encoded_center_scale_df %>% select(matches('^PBA\\.'), ELBTUPerSf))))

nn_input_df <- cbecs_elbtu_encoded_center_scale_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature),
         one_of(colnames(cbecs_elbtu_encoded_center_scale_df %>% select(matches('^PBA\\.'), ELBTUPerSf))))

nn_input_pba_labels <- cbecs_el_cleaned_df$PBA
  
save(nn_input_pba_labels, 
     nn_input_df, 
     elbtu_pre_process,
     file='nn_input.RData'
)
```


```{r echo=FALSE, fig.width=10}
control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid')
tunegrid <- expand.grid(size = seq(10, 210, 100), dropout = seq(0, 0.9, 0.45), batch_size = seq(1000, 2000, 1000), lr = seq(0.3, 0.7, 0.2), rho = seq(0.3, 0.7, 0.2), decay = seq(0.3, 0.7, 0.2), activation = c('relu'))
nn_train <- caret::train(
  y = nn_input_df$ELBTUPerSf,
  x = nn_input_df %>% select(-ELBTUPerSf),
  method='mlpKerasDropout',
  metric='RMSE',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv')
)
print(nn_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(nn_train, newx = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df)),
           y=ELBTUPerSf, color = cbecs_el_cleaned_df[cbecs_train_list,c("PBA")])) + geom_point()
varImp(nn_train)
```


Multi-layer Perceptron
```{r eval=FALSE}
#Takes a long time
control <- trainControl(index = cbecs_train_cv_list, method = 'repeatedcv', number = folds, repeats = 1, search = 'grid')
tunegrid <- expand.grid(layer1 = seq(10, 100, 50), layer2 = seq(10, 100, 50), layer3 = seq(10, 100, 50), decay = seq(0.2, 0.6, 0.4))
ml_nn_train <- caret::train(
  y = nn_input_df$ELBTUPerSf,
  x = nn_input_df %>% select(-ELBTUPerSf),
  method='mlpWeightDecayML',
  metric='RMSE',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv')
)
print(ml_nn_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(ml_nn_train, newx = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df)),
           y=ELBTUPerSf)) + geom_point()
varImp(ml_nn_train)
```


#Stop parallel processing
```{r}
stopCluster(cl)
```

