---
title: "cbecs_electrical_analysis"
author: "John Grando"
date: "February 11, 2019"
output: html_document
---

Source files
```{r}
source('tree_func.R')
source('cbecs_2012_clean_transform.R')
```

Libraries
```{r}
library(factoextra)
library(pls)
library(randomForest)
library(inTrees)
```

Load and transform
```{r}
cbecs_raw_df <- read.csv('./2012_public_use_data_aug2016.csv', 
                         header = TRUE, 
                         stringsAsFactors = FALSE)
cbecs_dfs <- clean_encode_cbecs(cbecs_raw_df)
```

### Explore


Response Variable
```{r}
lmbda <- BoxCoxTrans(cbecs_dfs$clean_df$ELBTUPerSf + 1)$lambda
ggplot(cbecs_dfs$clean_df, aes(x=ELBTUPerSf^lmbda)) + 
  geom_density() + 
  stat_function(fun=dnorm,
                color="blue",
                args=list(
                  mean=mean(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE),
                  sd(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE)
                )
  ) +
  ggtitle(paste('ELBTUPerSf Transformed via BoxCox, Lambda = ', lmbda, sep=""))
```

Remove highly correlated pairs and report???
```{r}
cbecs_elbtu_reduced_df <- cbecs_dfs$encoded_df %>% 
  select(-one_of(cbecs_dfs$response_cols)) %>% 
  select(findCorrelation(cor(.), cutoff=0.75, names = TRUE)) %>% 
  bind_cols(ELBTUPerSf = cbecs_dfs$encoded_df[,c('ELBTUPerSf')])
```

```{r}
cbecs_elbtu_df <- cbecs_dfs$encoded_df %>% 
  select(-one_of(cbecs_dfs$response_cols)) %>% 
  bind_cols(ELBTUPerSf = cbecs_dfs$encoded_df[,c('ELBTUPerSf')])
```

PCA
```{r fig.width=10}
cbecs_pca <- prcomp(cbecs_elbtu_reduced_df %>% 
    select(-ELBTUPerSf) %>%
    select(-one_of(cbecs_dfs$response_cols)), center = TRUE, scale. = TRUE)
screeplot(cbecs_pca, type = 'lines', npcs = 50)
var <- get_pca_var(cbecs_pca)
fviz_pca_var(cbecs_pca, select.var = list(contrib=50), repel = TRUE)
fviz_contrib(cbecs_pca, choice = "var", axes = 1:5, top = 50)
```


#Tree regression importance
```{r fig.height=10}
control <- trainControl(method = 'repeatedcv', number = 3, repeats = 2, search = 'grid')
tunegrid <- expand.grid(mtry = seq(10, 20, 20), maxdepth = seq(3, 4, 2))
pls_train <- train(
  ELBTUPerSf ~ .,
  data=cbecs_elbtu_reduced_df,
  method='rfRules',
  metric='Rsquared',
  tuneGrid=tunegrid,
  trControl=control
)
print(pls_train)
ggplot(cbecs_elbtu_df, 
       aes(x=pls_train$finalModel$fitted.values[,,26], 
           y=ELBTUPerSf)) + geom_point()
plot(varImp(pls_train), 40)
```



```{r fig.height=14}
control <- trainControl(method = 'repeatedcv', number = 5, repeats = 3, search = 'grid')
tunegrid <- expand.grid(ncomp = seq(1, 20, 2))
pls_train <- train(
  ELBTUPerSf^0.2 ~ .,
  data=cbecs_elbtu_reduced_df,
  method='pls',
  metric='Rsquared',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c("center", "scale")
)
print(pls_train)
ggplot(cbecs_elbtu_reduced_df, 
       aes(x=pls_train$finalModel$fitted.values[,,19]^(1/0.2), 
           y=ELBTUPerSf)) + geom_point()
plot(varImp(pls_train), 40)
```

rfe
```{r}
#Do rfe (recursive feature elimination) for linear model, and classification tree, then compare their metrics together.  Mabye set up a custom one or also try lasso?
```

