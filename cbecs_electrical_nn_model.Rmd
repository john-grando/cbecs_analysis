---
title: "multi_layer_perceptron"
author: "John Grando"
date: "February 13, 2019"
output: html_document
---

Load libraries
```{r}
library(dplyr)
library(doParallel)
library(tidyr)
library(caret)
library(keras)
library(tibble)
```

#Load input data and assign as train/test/validation
```{r}
load('nn_elbtu_input.RData')
#stratify training set
train_test_list <- createDataPartition(y=nn_input_pba_labels, 
                                        p=0.8, 
                                        list = FALSE)
train_test_df <- nn_input_df %>% 
  bind_cols(PBA=nn_input_pba_labels) %>% 
  slice(train_test_list)

train_list <- createDataPartition(y=train_test_df$PBA,
                                  p=0.8,
                                  list=FALSE)
train_df <- as.matrix(train_test_df %>% 
  slice(train_list) %>% 
  select(-PBA, -ELBTUPerSf))

train_labels <- (train_test_df %>% 
  slice(train_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf

test_df <- as.matrix(train_test_df %>% 
  slice(-train_list) %>% 
  select(-PBA, -ELBTUPerSf))

test_labels <- (train_test_df %>% 
  slice(-train_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf

validation_df <- as.matrix(nn_input_df %>% 
  slice(-train_test_list) %>% 
  select(-ELBTUPerSf))

validation_labels <- (nn_input_df %>%
  slice(-train_test_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf
```

#Initialize parallel processing on 2 cores
```{r}
cl <- makePSOCKcluster(2)
registerDoParallel(cl)
```

#Custom loss funciton
```{r}
custom_loss_func <- function(y_true, y_pred) {
  K <- backend()
  first_log <- K$log(K$relu(y_pred)+1)
  second_log <- K$log(K$relu(y_true)+1)
  K$mean(K$square(first_log - second_log) / K$clip(K$abs(second_log),1,700))
}
```


#Build keras model
```{r}
build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 400, 
                activation = "relu",
                input_shape = dim(train_df)[2]
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.6) %>%
    layer_dense(units = 400, 
                activation = "relu"
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.6) %>%
    layer_dense(units = 400, 
                activation = "relu"
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.2) %>%
    layer_dense(units = 400, 
                activation = "relu"
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.2) %>%
    layer_dense(units = 400, 
                activation = "relu"
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.2) %>%
    layer_dense(units = 400, 
                activation = "relu"
                #kernel_regularizer = regularizer_l2()
                ) %>%
    layer_dropout(0.2) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    #loss = keras::loss_mean_squared_logarithmic_error,
    loss = custom_loss_func,
    optimizer = keras::optimizer_sgd(decay = 0.05),
    #optimizer = keras::optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  model
}

model <- build_model()
model %>% summary()
```

#Function for model building
```{r}
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 50)
```

#Run model
```{r}
epochs <- 500

# Fit the model and store training stats
history <- model %>% fit(
  train_df,
  train_labels,
  epochs = epochs,
  validation_data = list(test_df, test_labels),
  batch_size = 1000,
  verbose = 1,
  callbacks = list(print_dot_callback, early_stop)
)
```

#Stop parallel processing
```{r}
stopCluster(cl)
```

#Predict and examine residuals
```{r fig.width=10}
plot(history)
evaluation <- model %>% evaluate(validation_df, validation_labels, verbose=2)
evaluation
resid_model <- lm(model %>% predict(validation_df)~validation_labels)
summary(resid_model)
residuals <- resid(resid_model)
resid_df <- data.frame(resid = residuals, 
                       pba = data.frame(pba=nn_input_pba_labels) %>% slice(-train_test_list),
                       elbtu_per_sf = validation_labels)
ggplot(data = data.frame(pred=model %>% predict(validation_df),
                         obs=validation_labels),
       aes(x=pred, y=obs)) + 
  geom_point() + 
  geom_abline(slope = 1)
ggplot(resid_df, aes(x=resid)) + geom_density() + facet_wrap(~pba, scales = 'free')
ggplot(resid_df, aes(x=elbtu_per_sf, y=resid)) + geom_point()
```

