---
title: "multi_layer_perceptron"
author: "John Grando"
date: "February 13, 2019"
output: html_document
---

Load libraries
```{r}
library(dplyr)
library(doParallel)
library(tidyr)
library(caret)
library(keras)
library(tibble)
library(tensorflow)
library(foreach)
library(doMC)
```

#Load input data and assign as train/test/validation
```{r}
<<<<<<< HEAD
load('nn_elbtu_input.RData')
=======
load('ModelSaves/elbtu_nn_input.RData')
>>>>>>> 3cb970c39f2115dae15235ea0999d4d9975d8f60
#stratify training set
train_test_list <- createDataPartition(y=nn_input_pba_labels, 
                                        p=0.8, 
                                        list = FALSE)
train_test_df <- nn_input_df %>% 
  bind_cols(PBA=nn_input_pba_labels) %>% 
  slice(train_test_list)

train_list <- createDataPartition(y=train_test_df$PBA,
                                  p=0.8,
                                  list=FALSE)
train_df <- train_test_df %>% 
  slice(train_list) %>% 
  select(-PBA, -ELBTUPerSf)

train_labels <- (train_test_df %>% 
  slice(train_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf

test_df <- train_test_df %>% 
  slice(-train_list) %>% 
  select(-PBA, -ELBTUPerSf)

test_labels <- (train_test_df %>% 
  slice(-train_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf

validation_df <- nn_input_df %>% 
  slice(-train_test_list) %>% 
  select(-ELBTUPerSf)

validation_labels <- (nn_input_df %>%
  slice(-train_test_list) %>% 
  select(ELBTUPerSf))$ELBTUPerSf
```

#Custom loss funciton
```{r}
custom_loss_func <- function(y_true, y_pred) {
  K <- backend()
  second_y <- tf$multiply(K$relu(y_true), 0.01)
  K$mean(tf$multiply(K$square(y_pred - y_true) / K$clip(y_true,0.1,1000), second_y))
}

percentage_metric <- custom_metric('percentage_metric', function(y_true, y_pred){
  K <- backend()
  K$mean(tf$multiply(K$abs(1 - K$abs(y_true - y_pred) / K$clip(y_true,0.1,1000)), y_true))
})
```


#Build 3h model
```{r}
build_3h_model <- function(df = NA, n_dropout=0, n_units=10, n_l = 0) {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = n_units, 
                activation = "relu",
                input_shape = dim(df)[2],
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = n_units, 
                activation = "relu",
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = n_units, 
                activation = "relu",
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = 'mse',
    #loss = keras::loss_mean_squared_logarithmic_error,
    #loss = custom_loss_func,
    #optimizer = keras::optimizer_sgd(lr = 0.01, decay = 0.0),
    optimizer = keras::optimizer_rmsprop(),
    metrics = list("mean_absolute_error", percentage_metric)
  )
  model
  
}
```


#build 2h model
```{r}
build_2h_model <- function(df = NA, n_dropout=0, n_units=10, n_l = 0) {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = n_units, 
                activation = "relu",
                input_shape = dim(df)[2],
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = n_units, 
                activation = "relu",
                input_shape = dim(df)[2],
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = keras::loss_mean_squared_logarithmic_error,
    #loss = custom_loss_func,
    optimizer = keras::optimizer_sgd(lr = 0.01, decay = 0.0),
    #optimizer = keras::optimizer_rmsprop(),
    metrics = list("mean_absolute_error", percentage_metric)
  )
  
  model
}
```


#build 1h model
```{r}
build_1h_model <- function(df = NA, n_dropout=0, n_units=10, n_l = 0) {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = n_units, 
                activation = "relu",
                input_shape = dim(df)[2],
                kernel_regularizer = regularizer_l2(l = n_l)
                ) %>%
    layer_dropout(n_dropout) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = keras::loss_mean_squared_logarithmic_error,
    #loss = custom_loss_func,
    optimizer = keras::optimizer_sgd(lr = 0.01, decay = 0.0),
    #optimizer = keras::optimizer_rmsprop(),
    metrics = list("mean_absolute_error", percentage_metric)
  )
  
  model
}
```

#build 0h model
```{r}
build_0h_model <- function(df = NA, n_dropout=0, n_units=10, n_l = 0) {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 1, 
                activation = "relu",
                input_shape = dim(df)[2],
                kernel_regularizer = regularizer_l2(l = n_l)
                )
  
  model %>% compile(
    loss = keras::loss_mean_squared_logarithmic_error,
    #loss = custom_loss_func,
    optimizer = keras::optimizer_sgd(lr = 0.01, decay = 0.0),
    #optimizer = keras::optimizer_rmsprop(),
    metrics = list("mean_absolute_error", percentage_metric)
  )
  
  model
}
```

#model selector
```{r}
model_selector <- function(model_n = '1', ...){
  model_n <- as.character(model_n)
  m_selected <- 0
  if(model_n=='0'){
    x <- build_0h_model(...)
    m_selected <- 1
  }
  if(model_n=='1'){
    x <- build_1h_model(...)
    m_selected <- 1
  }
  if(model_n=='2'){
    x <- build_2h_model(...)
m_selected <- 1
  }
  if(model_n=='3'){
    x <- build_3h_model(...)
    m_selected <- 1
  }
  if(m_selected==0){stop('model does not exist')}
  return(x)
}
```

#Function for model building
```{r}
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 50)
```

#Hyperparameter training
```{r}
n_folds <- 4
folds <- createFolds(y = train_test_list, k=n_folds, list=FALSE)

hyper_list <- list()
hyper_list$dropout <- seq(0, 0.6, 0.3)
hyper_list$units <- seq(10, 110, 25)
hyper_list$regularizer <- seq(0, 0.2, 0.1)
hyper_list$model <- seq(0,3,1)
```


#Initialize parallel processing on 2 cores
```{r}
cl <- makeCluster(2)
registerDoMC(2)
```

#Run model
```{r echo=FALSE}
epochs <- 100
hyper_results <- data.frame()
for (m in hyper_list$model) {
  for (r in hyper_list$regularizer) {
    for (d in hyper_list$dropout) {
      for (u in hyper_list$units) {
        tmp_df <- foreach(f = 1:n_folds,
                       .combine = rbind) %dopar% {
          #print(paste('running dropout, units, regularizer, fold:',d,u,r,f, sep=' '))
          ind <- which(folds == f)
          cv_train_df <- train_test_df %>% slice(-ind) %>% select(-PBA, -ELBTUPerSf)
          cv_train_y <- train_test_df %>% slice(-ind) %>% select(ELBTUPerSf)
          cv_test_df <- train_test_df %>% slice(ind) %>% select(-PBA, -ELBTUPerSf)
          cv_test_y <- train_test_df %>% slice(ind) %>% select(ELBTUPerSf)
          cv_model_t <- model_selector(model_n = m, df = train_df, n_dropout = d, n_units = u, n_l=r)
          history <- cv_model_t %>% fit(
            as.matrix(cv_train_df),
            as.matrix(cv_train_y),
            epochs = epochs,
            validation_data = list(as.matrix(cv_test_df), as.matrix(cv_test_y)),
            batch_size = 1000,
            verbose = 0)
          loss_val <- tail(history$metrics$val_loss,1)
          mae_val <- tail(history$metrics$val_mean_absolute_error,1)
          pm_val <- tail(history$metrics$percentage_metric,1)
          #hyper_results <- rbind(hyper_results, data.frame(dropout=d, units=u, fold=f, loss= loss_val, mae = mae_val, pm = pm_val))
          data.frame(model=m, dropout=d, units=u, reg=r, fold=f, loss= loss_val, mae = mae_val, pm = pm_val)
        }
      print(paste('finished running model: model, droput, units, regularizer', m, d, u, r, sep = " "))
      hyper_results <- rbind(hyper_results, tmp_df)
      }
    }
  }
}
  
hyper_summary <- hyper_results %>% 
  group_by(model, dropout, units, reg) %>% 
  summarize(loss = mean(loss), 
            mae = mean(mae), 
            pm = mean(pm))
```

#Stop parallel processing
```{r}
stopCluster(cl)
```

```{r eval=FALSE}
# Fit the model with best hyper parameters and store training stats
model <- build_3h_model(df=train_df, n_dropout=0.3, n_units = 60, n_l = 0)
model %>% summary()
epochs <- 500
history <- model %>% fit(
  as.matrix(train_df),
  as.matrix(train_labels),
  epochs = epochs,
  validation_data = list(as.matrix(test_df), as.matrix(test_labels)),
  batch_size = 1000,
  verbose = 1,
  callbacks = list(print_dot_callback, early_stop)
)
```

#Predict and examine residuals
```{r eval=FALSE, fig.width=10}
plot(history)
evaluation <- model %>% evaluate(as.matrix(validation_df), as.matrix(validation_labels), verbose=2)
evaluation
resid_model <- lm(model %>% predict(as.matrix(validation_df))~as.matrix(validation_labels))
summary(resid_model)
par(mfrow=c(2,2))
plot(resid_model)
residuals <- resid(resid_model)
resid_df <- data.frame(resid = residuals, 
                       pba = data.frame(pba=nn_input_pba_labels) %>% slice(-train_test_list),
                       elbtu_per_sf = validation_labels,
                       pred = model %>% predict(as.matrix(validation_df)),
                       ratio = abs(1 - (1 + model %>% predict(as.matrix(validation_df))) / (1 + validation_labels)))
ggplot(data = data.frame(pred=model %>% predict(as.matrix(validation_df)),
                         obs=validation_labels),
       aes(x=pred, y=obs)) + 
  geom_point() + 
  geom_abline(slope = 1)
#ggplot(resid_df, aes(x=resid)) + geom_density() + facet_wrap(~pba, scales = 'free')
ggplot(resid_df, aes(x=elbtu_per_sf, y=resid)) + geom_point()

elbtu_impact_error_check_df <- resid_df %>% 
  group_by(pba) %>% 
  summarize(avg_ratio = mean(ratio), 
            elbtu_per_sf_error = mean(ratio * elbtu_per_sf))
```

