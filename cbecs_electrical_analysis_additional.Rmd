---
title: "cbecs_electrical_analysis"
author: "John Grando"
date: "February 11, 2019"
output: html_document
---

Source files
```{r}
source('tree_func.R')
source('cbecs_2012_clean_transform.R')
```

Libraries
```{r}
library(factoextra)
library(pls)
library(randomForest)
library(inTrees)
library(doParallel)
library(elasticnet)
library(keras)
library(tibble)
library(RSNNS)
```
#Initialize parallel processing on 2 cores
```{r}
cl <- makePSOCKcluster(1)
registerDoParallel(cl)
```

Load and transform
```{r}
cbecs_raw_df <- read.csv('./2012_public_use_data_aug2016.csv', 
                         header = TRUE, 
                         stringsAsFactors = FALSE)
cbecs_dfs <- clean_encode_cbecs(cbecs_raw_df, 34)
```

### Explore

```{r}
#remove buildings that know the don't use electricity
cbecs_el_encoded_df <- cbecs_dfs$encoded_df %>% 
  filter(ELUSED.1==1)
cbecs_el_cleaned_df <- cbecs_dfs$clean_df %>% 
  filter(ELUSED==1)
```

Response Variable
```{r}
lmbda <- BoxCoxTrans(cbecs_el_cleaned_df$ELBTUPerSf + 1)$lambda
lmbda <- ifelse(lmbda==0, 1, lmbda)
ggplot(cbecs_el_cleaned_df, aes(x=ELBTUPerSf^lmbda)) + 
  geom_density() + 
  stat_function(fun=dnorm,
                color="blue",
                args=list(
                  mean=mean(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE),
                  sd(cbecs_dfs$clean_df$ELBTUPerSf^lmbda, na.rm = TRUE)
                )
  ) +
  ggtitle(paste('ELBTUPerSf Transformed via BoxCox, Lambda = ', lmbda, sep=""))

```

Remove highly correlated pairs and report???
```{r warning=FALSE}
cbecs_elbtu_encoded_df <- cbecs_el_encoded_df %>% 
  #remove zero variance variables
  select(-one_of(preProcess(., method = c('zv'))$method$remove)) %>% 
  #remove response columns
  select(-one_of(cbecs_dfs$response_cols)) %>%
  #remove pba
  select(-matches('^PBA\\.')) %>% 
  select(-one_of(findCorrelation(cor(.), cutoff=0.75, names = TRUE))) %>%
  #remove remaining pba values then reinsert all
  #select(-matches('^PBAPLUS')) %>% 
  #bind_cols(cbecs_el_encoded_df %>% select(matches('^PBAPLUS'))) %>% 
  bind_cols(ELBTUPerSf = cbecs_el_encoded_df %>% select(ELBTUPerSf))

findCorrelation(cor(cbecs_elbtu_encoded_df), cutoff=0.75, names = TRUE)

cbecs_elbtu_encoded_numerics_cols <- colnames(
  cbecs_elbtu_encoded_df %>% 
    select(one_of(cbecs_dfs$encoded_numeric_cols)) %>% 
    select(-ELBTUPerSf)) 
cbecs_elbtu_encoded_non_numerics_cols <- colnames(
  cbecs_elbtu_encoded_df %>% 
  select(one_of(cbecs_dfs$encoded_non_numeric_cols)))

#center and scale only numeric columns in data set, not encoder columns
elbtu_pre_process <- preProcess(
  cbecs_elbtu_encoded_df, 
  method = list(center = cbecs_elbtu_encoded_numerics_cols, 
       scale = cbecs_elbtu_encoded_numerics_cols))
cbecs_elbtu_encoded_center_scale_df <- predict(elbtu_pre_process, cbecs_elbtu_encoded_df)
```

#Set number of most important variables to take from each model
```{r}
top_n_rows <- 5
```

PCA
```{r fig.width=10}
cbecs_pca <- prcomp(cbecs_elbtu_encoded_center_scale_df %>% 
    select(-ELBTUPerSf) %>%
    select(-one_of(cbecs_dfs$response_cols)), center = TRUE, scale. = TRUE)
fviz_eig(cbecs_pca, addlabels = TRUE)
var <- get_pca_var(cbecs_pca)
fviz_pca_var(cbecs_pca, select.var = list(contrib=50), repel = TRUE)
fviz_contrib(cbecs_pca, choice = "var", axes = 1:5, top = 50)
pca_imp_df <- fviz_contrib(cbecs_pca, choice = "var", axes = 1:5, top = 50)$data %>% arrange(desc(contrib)) %>% head(top_n_rows) %>% select(name) %>% rename(feature = name) %>% mutate(feature = as.character(feature))
```

#Train/Test split
```{r}
cbecs_train_list <- createDataPartition(y= cbecs_el_cleaned_df$PBAPLUS, p=0.8, list = FALSE) 
cbecs_elbtu_encoded_train_df <- cbecs_elbtu_encoded_df[cbecs_train_list,]
cbecs_elbtu_encoded_test_df <- cbecs_elbtu_encoded_df[-cbecs_train_list,]
cbecs_elbtu_encoded_center_scale_train_df <- cbecs_elbtu_encoded_center_scale_df[cbecs_train_list,]
cbecs_elbtu_encoded_center_scale_test_df <- cbecs_elbtu_encoded_center_scale_df[-cbecs_train_list,]
#stratify training set
folds <- 2
cbecs_train_cv_list <- createFolds(y=cbecs_el_cleaned_df[cbecs_train_list,]$PBAPLUS, k = folds, returnTrain = TRUE)
```

#PLS 
```{r fig.height=14}
control <- trainControl(index = cbecs_train_cv_list, 
                        method = 'repeatedcv', 
                        number = folds, 
                        repeats = 1, 
                        search = 'grid', 
                        verboseIter = TRUE)
tunegrid <- expand.grid(ncomp = seq(1, 30, 2))
pls_train <- caret::train(
  y = cbecs_elbtu_encoded_center_scale_train_df$ELBTUPerSf^lmbda,
  x = cbecs_elbtu_encoded_center_scale_train_df %>% select(-ELBTUPerSf),
  method='pls',
  metric='RMSE',
  maximize=FALSE,
  tuneGrid=tunegrid,
  trControl=control
)
print(pls_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=pls_train$finalModel$fitted.values[,,1]^(1/lmbda), 
           y=ELBTUPerSf)) + geom_point()
varImp(pls_train)
pls_imp_df <- varImp(pls_train)$importance %>% 
  rownames_to_column(var='feature') %>% 
  arrange(desc(Overall)) %>% 
  head(top_n_rows) %>% 
  select(feature)

plot(pls_train$finalModel$fitted.values[,,1]^(1/lmbda), pls_train$finalModel$residuals[,,1])
```

#Tree regression importance
```{r fig.height=10}
#eval set to false, takes a long time to process
control <- trainControl(index = cbecs_train_cv_list, 
                        method = 'repeatedcv', 
                        number = folds, 
                        repeats = 1, 
                        search = 'grid', 
                        verboseIter = TRUE)
tunegrid <- expand.grid(mtry = seq(100, 300, 100))
rf_train <- caret::train(
  y = cbecs_elbtu_encoded_train_df$ELBTUPerSf,
  x = cbecs_elbtu_encoded_train_df %>% select(-ELBTUPerSf),
  method='rf',
  metric='RMSE',
  tuneGrid=tunegrid,
  trControl=control,
  maxnodes=30,
  importance = TRUE
)
print(rf_train)
ggplot(cbecs_elbtu_encoded_train_df, 
       aes(x=rf_train$finalModel$predicted, 
           y=ELBTUPerSf)) + geom_point()
varImp(rf_train)
rf_imp_df <- varImp(rf_train)$importance %>% 
  rownames_to_column(var='feature') %>% 
  arrange(desc(Overall)) %>% 
  head(top_n_rows) %>% 
  select(feature)
```

lasso
```{r eval=FALSE}
#This doesn't work and takes too long, use it as a reason for nn model

control <- trainControl(index = cbecs_train_cv_list, 
                        method = 'repeatedcv', 
                        number = folds, 
                        repeats = 1, 
                        search = 'grid', 
                        verboseIter = TRUE)
tunegrid <- expand.grid(fraction = seq(.01, .03, .01))
l_train <- caret::train(
  y = cbecs_elbtu_encoded_center_scale_train_df$ELBTUPerSf^lmbda,
  x = cbecs_elbtu_encoded_center_scale_train_df %>% select(-ELBTUPerSf),
  method='lasso',
  metric='RMSE',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv'),
  weights = 1 / (cbecs_elbtu_encoded_center_scale_train_df$ELBTUPerSf^lmbda)
)
print(l_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(l_train$finalModel, newdata = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df))^(1/lmbda),
           y=ELBTUPerSf)) + geom_point()
varImp(l_train)
```

leaps
```{r eval=FALSE}
#This doesnt work and takes too long, use ti as as reason for nn model

#Do lasso as well, then maybe try rfe (recursive feature elimination) for linear model, and classification tree, then compare their metrics together.  Mabye set up a custom one or also try lasso?

control <- trainControl(index = cbecs_train_cv_list, 
                        method = 'repeatedcv', 
                        number = folds, 
                        repeats = 1, 
                        search = 'grid', 
                        verboseIter = TRUE)
tunegrid <- expand.grid(nvmax = seq(10, 30, 10))
l_train <- caret::train(
  y = cbecs_elbtu_encoded_center_scale_train_df$ELBTUPerSf^lmbda,
  x = cbecs_elbtu_encoded_center_scale_train_df %>% select(-ELBTUPerSf),
  method='leapBackward',
  metric='RMSE',
  tuneGrid=tunegrid,
  trControl=control,
  preProcess = c('zv')
)
print(l_train)
ggplot(cbecs_elbtu_encoded_center_scale_train_df, 
       aes(x=predict(l_train, newx = predict(elbtu_pre_process, newdata = cbecs_elbtu_encoded_center_scale_train_df))^(1/lmbda),
           y=ELBTUPerSf)) + geom_point()
varImp(l_train)
```

Plots for high importance variables
```{r}
ggplot(cbecs_el_cleaned_df, aes(x=(RGSTRNPerSf+1), y=ELBTUPerSf^lmbda)) + geom_point()
#also do this for residuals on best fits...
```

#make neural network input df based on the most important features selected
```{r}
nn_input_train_test_df <- cbecs_elbtu_encoded_center_scale_train_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature),
         one_of(colnames(cbecs_elbtu_encoded_center_scale_df %>% select(matches('^PBAPLUS'), ELBTUPerSf))))

nn_input_validate_df <- cbecs_elbtu_encoded_center_scale_test_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature),
         one_of(colnames(cbecs_elbtu_encoded_center_scale_df %>% select(matches('^PBAPLUS'), ELBTUPerSf))))
```

#Save dataframes for full analysis
```{r}
load('nn_elbtu_input.RData')
nn_input_add_df <- cbecs_elbtu_encoded_center_scale_df %>% 
  select(one_of(pls_imp_df$feature), 
         one_of(pca_imp_df$feature),
         one_of(rf_imp_df$feature))

(nn_input_add_df_cols_tmp <- colnames(nn_input_add_df %>% select(-one_of(colnames(nn_input_df)))))
```

#Stop parallel processing
```{r}
stopCluster(cl)
```

